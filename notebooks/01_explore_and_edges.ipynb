{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d6041e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment\n",
      "-----------\n",
      "Python        : 3.11.14  (Darwin 24.6.0)\n",
      "pandas        : 2.2.2\n",
      "numpy         : 1.26.4\n",
      "networkx      : 3.3\n",
      "matplotlib    : 3.8.4\n",
      "scikit-learn  : 1.4.2\n",
      "xgboost       : 2.0.3\n",
      "\n",
      "Paths\n",
      "-----\n",
      "DATA_ROOT     : /Users/ramana/Documents/Homework/2nd class Network Analytics/Project 2/Influence-Analysis/data/raw/SAGraph\n",
      "OK: DATA_ROOT exists.\n"
     ]
    }
   ],
   "source": [
    "# 01_catalog_and_peek.ipynb — Cell 1\n",
    "# Environment check: imports, versions, and data path verification.\n",
    "# No data loading happens here.\n",
    "\n",
    "from pathlib import Path\n",
    "import sys, platform\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib as mpl\n",
    "\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "\n",
    "# Repro-friendly display settings (safe, not heavy)\n",
    "pd.set_option(\"display.max_rows\", 20)\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "\n",
    "# Resolve data root (expected location for SAGraph files)\n",
    "DATA_ROOT = Path(\"data/raw/SAGraph\")\n",
    "\n",
    "# Print environment summary\n",
    "print(\"Environment\")\n",
    "print(\"-----------\")\n",
    "print(f\"Python        : {sys.version.split()[0]}  ({platform.system()} {platform.release()})\")\n",
    "print(f\"pandas        : {pd.__version__}\")\n",
    "print(f\"numpy         : {np.__version__}\")\n",
    "print(f\"networkx      : {nx.__version__}\")\n",
    "print(f\"matplotlib    : {mpl.__version__}\")\n",
    "print(f\"scikit-learn  : {sklearn.__version__}\")\n",
    "print(f\"xgboost       : {xgb.__version__}\")\n",
    "\n",
    "# Verify data folder exists (fail early if not)\n",
    "print(\"\\nPaths\")\n",
    "print(\"-----\")\n",
    "print(f\"DATA_ROOT     : {DATA_ROOT.resolve()}\")\n",
    "if not DATA_ROOT.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Expected SAGraph files under data/raw/SAGraph. \"\n",
    "        \"Create this folder and place product_info.jsonl and the *.graph.anon files there.\"\n",
    "    )\n",
    "else:\n",
    "    print(\"OK: DATA_ROOT exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e41e9942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_info overview\n",
      "---------------------\n",
      "rows: 6, columns: ['product_name', 'domain', 'ads', 'influencer_ids', 'influencer_ids_count']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>domain</th>\n",
       "      <th>ads</th>\n",
       "      <th>influencer_ids</th>\n",
       "      <th>influencer_ids_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spark_thinking</td>\n",
       "      <td>education</td>\n",
       "      <td>火花思维11月活动海报见图！4880元到手48节课，赠送1W5火花币，听力熊听力机，科普书《...</td>\n",
       "      <td>[162395, 172251, 177999, 171351, 163119]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>electric_toothbrush</td>\n",
       "      <td>hygiene</td>\n",
       "      <td>之前让高速吹风机更加普及的#徕芬#，这次又来普及电动牙刷了，“果味”实足的外观，扫振一体的设...</td>\n",
       "      <td>[146880, 136519, 67092, 132945, 70148, 139659,...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ruby_face_cream</td>\n",
       "      <td>skincare</td>\n",
       "      <td>红宝石面霜是我近几年用最多的面霜，超级爱用。升级后使用感更好了，用了之后感觉皮肤状态也很好。...</td>\n",
       "      <td>[31027, 39487, 1, 30328, 8724, 16798, 20277, 3...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          product_name     domain                                                ads  \\\n",
       "0       spark_thinking  education  火花思维11月活动海报见图！4880元到手48节课，赠送1W5火花币，听力熊听力机，科普书《...   \n",
       "1  electric_toothbrush    hygiene  之前让高速吹风机更加普及的#徕芬#，这次又来普及电动牙刷了，“果味”实足的外观，扫振一体的设...   \n",
       "2      ruby_face_cream   skincare  红宝石面霜是我近几年用最多的面霜，超级爱用。升级后使用感更好了，用了之后感觉皮肤状态也很好。...   \n",
       "\n",
       "                                      influencer_ids  influencer_ids_count  \n",
       "0           [162395, 172251, 177999, 171351, 163119]                     5  \n",
       "1  [146880, 136519, 67092, 132945, 70148, 139659,...                     8  \n",
       "2  [31027, 39487, 1, 30328, 8724, 16798, 20277, 3...                    14  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dtypes:\n",
      "product_name            object\n",
      "domain                  object\n",
      "ads                     object\n",
      "influencer_ids          object\n",
      "influencer_ids_count     int64\n",
      "dtype: object\n",
      "\n",
      "Cross-check\n",
      "-----------\n",
      "Products in files : 6 -> ['abc_reading', 'electric_toothbrush', 'intelligent_floor_scrubber', 'ruby_face_cream', 'spark_thinking', 'supor_boosted_showerhead']\n",
      "Products in info  : 6 -> ['abc_reading', 'electric_toothbrush', 'intelligent_floor_scrubber', 'ruby_face_cream', 'spark_thinking', 'supor_boosted_showerhead']\n",
      "Missing in info   : []\n",
      "Missing in files  : []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "info_file = DATA_ROOT / \"product_info.jsonl\"  # this is actually a JSON array\n",
    "\n",
    "# 1) Load entire file as a JSON array\n",
    "with info_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    info_list = json.load(f)   # list[dict] if file is a JSON array\n",
    "\n",
    "if not isinstance(info_list, list):\n",
    "    raise ValueError(\"Expected a JSON array in product_info.jsonl (found something else).\")\n",
    "\n",
    "info_df = pd.DataFrame(info_list)\n",
    "\n",
    "# 2) Normalize influencer_ids to a clean list[int]\n",
    "def to_int_list(x):\n",
    "    if isinstance(x, list):\n",
    "        vals = []\n",
    "        for v in x:\n",
    "            try:\n",
    "                vals.append(int(v))\n",
    "            except Exception:\n",
    "                continue\n",
    "        return vals\n",
    "    return []\n",
    "\n",
    "if \"influencer_ids\" in info_df.columns:\n",
    "    info_df[\"influencer_ids\"] = info_df[\"influencer_ids\"].apply(to_int_list)\n",
    "    info_df[\"influencer_ids_count\"] = info_df[\"influencer_ids\"].apply(len)\n",
    "else:\n",
    "    info_df[\"influencer_ids\"] = [[] for _ in range(len(info_df))]\n",
    "    info_df[\"influencer_ids_count\"] = 0\n",
    "\n",
    "# 3) Display basic structure\n",
    "print(\"product_info overview\")\n",
    "print(\"---------------------\")\n",
    "print(f\"rows: {len(info_df)}, columns: {list(info_df.columns)}\")\n",
    "display(info_df.head(3))\n",
    "print(\"\\ndtypes:\")\n",
    "print(info_df.dtypes)\n",
    "\n",
    "# 4) Cross-check product names vs files we discovered\n",
    "prof_names = {p.name.replace(\"_profile.graph.anon\", \"\") for p in DATA_ROOT.glob(\"*_profile.graph.anon\")}\n",
    "inter_names = {p.name.replace(\"_interaction.graph.anon\", \"\") for p in DATA_ROOT.glob(\"*_interaction.graph.anon\")}\n",
    "both_files = sorted(prof_names & inter_names)\n",
    "\n",
    "names_from_info = sorted(info_df[\"product_name\"].dropna().unique().tolist())\n",
    "\n",
    "print(\"\\nCross-check\")\n",
    "print(\"-----------\")\n",
    "print(f\"Products in files : {len(both_files)} -> {both_files}\")\n",
    "print(f\"Products in info  : {len(names_from_info)} -> {names_from_info}\")\n",
    "\n",
    "missing_in_info = sorted(set(both_files) - set(names_from_info))\n",
    "missing_in_files = sorted(set(names_from_info) - set(both_files))\n",
    "print(f\"Missing in info   : {missing_in_info}\")\n",
    "print(f\"Missing in files  : {missing_in_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4aab0021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== abc_reading ===\n",
      "{'product': 'abc_reading', 'users': 70257, 'edges': 439620, 'engaged_users': 9068, 'self_loops_removed': 18893, 'top_in_degree': 5676}\n",
      "\n",
      "=== electric_toothbrush ===\n",
      "{'product': 'electric_toothbrush', 'users': 43610, 'edges': 161551, 'engaged_users': 2758, 'self_loops_removed': 1465, 'top_in_degree': 9527}\n",
      "\n",
      "=== intelligent_floor_scrubber ===\n",
      "{'product': 'intelligent_floor_scrubber', 'users': 63618, 'edges': 356506, 'engaged_users': 7556, 'self_loops_removed': 13670, 'top_in_degree': 5341}\n",
      "\n",
      "=== ruby_face_cream ===\n",
      "{'product': 'ruby_face_cream', 'users': 66247, 'edges': 121861, 'engaged_users': 4871, 'self_loops_removed': 4998, 'top_in_degree': 7963}\n",
      "\n",
      "=== spark_thinking ===\n",
      "{'product': 'spark_thinking', 'users': 58509, 'edges': 473594, 'engaged_users': 9616, 'self_loops_removed': 16837, 'top_in_degree': 8159}\n",
      "\n",
      "=== supor_boosted_showerhead ===\n",
      "{'product': 'supor_boosted_showerhead', 'users': 138424, 'edges': 241365, 'engaged_users': 8355, 'self_loops_removed': 6376, 'top_in_degree': 16777}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>users</th>\n",
       "      <th>edges</th>\n",
       "      <th>engaged_users</th>\n",
       "      <th>self_loops_removed</th>\n",
       "      <th>top_in_degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>supor_boosted_showerhead</td>\n",
       "      <td>138424</td>\n",
       "      <td>241365</td>\n",
       "      <td>8355</td>\n",
       "      <td>6376</td>\n",
       "      <td>16777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abc_reading</td>\n",
       "      <td>70257</td>\n",
       "      <td>439620</td>\n",
       "      <td>9068</td>\n",
       "      <td>18893</td>\n",
       "      <td>5676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ruby_face_cream</td>\n",
       "      <td>66247</td>\n",
       "      <td>121861</td>\n",
       "      <td>4871</td>\n",
       "      <td>4998</td>\n",
       "      <td>7963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>intelligent_floor_scrubber</td>\n",
       "      <td>63618</td>\n",
       "      <td>356506</td>\n",
       "      <td>7556</td>\n",
       "      <td>13670</td>\n",
       "      <td>5341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spark_thinking</td>\n",
       "      <td>58509</td>\n",
       "      <td>473594</td>\n",
       "      <td>9616</td>\n",
       "      <td>16837</td>\n",
       "      <td>8159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>electric_toothbrush</td>\n",
       "      <td>43610</td>\n",
       "      <td>161551</td>\n",
       "      <td>2758</td>\n",
       "      <td>1465</td>\n",
       "      <td>9527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      product   users   edges  engaged_users  self_loops_removed  top_in_degree\n",
       "5    supor_boosted_showerhead  138424  241365           8355                6376          16777\n",
       "0                 abc_reading   70257  439620           9068               18893           5676\n",
       "3             ruby_face_cream   66247  121861           4871                4998           7963\n",
       "2  intelligent_floor_scrubber   63618  356506           7556               13670           5341\n",
       "4              spark_thinking   58509  473594           9616               16837           8159\n",
       "1         electric_toothbrush   43610  161551           2758                1465           9527"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Batch across all products: build edges, clean self-loops, compute network features,\n",
    "# and save interim files for each product. Prints a short summary per product.\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "root = DATA_ROOT\n",
    "products = sorted({\n",
    "    p.name.replace(\"_profile.graph.anon\", \"\") for p in root.glob(\"*_profile.graph.anon\")\n",
    "} & {\n",
    "    p.name.replace(\"_interaction.graph.anon\", \"\") for p in root.glob(\"*_interaction.graph.anon\")\n",
    "})\n",
    "\n",
    "batch_summary = []\n",
    "\n",
    "for prod in products:\n",
    "    print(f\"\\n=== {prod} ===\")\n",
    "\n",
    "    # Load profiles (for followers/friends join later)\n",
    "    with (root / f\"{prod}_profile.graph.anon\").open(\"r\", encoding=\"utf-8\") as f:\n",
    "        prof_obj = json.load(f)\n",
    "    prof_df = pd.DataFrame.from_dict(prof_obj, orient=\"index\").reset_index(drop=True)\n",
    "    for col in [\"user_id\", \"user_followers\", \"user_friends\"]:\n",
    "        if col not in prof_df.columns:\n",
    "            prof_df[col] = pd.NA\n",
    "    prof_df[\"user_id\"] = pd.to_numeric(prof_df[\"user_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    prof_df[\"user_followers\"] = pd.to_numeric(prof_df[\"user_followers\"], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
    "    prof_df[\"user_friends\"] = pd.to_numeric(prof_df[\"user_friends\"], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
    "\n",
    "    # Load interactions → edges\n",
    "    with (root / f\"{prod}_interaction.graph.anon\").open(\"r\", encoding=\"utf-8\") as f:\n",
    "        inter_obj = json.load(f)\n",
    "\n",
    "    rows = []\n",
    "    for key_user_str, lst in inter_obj.items():\n",
    "        try:\n",
    "            dst = int(key_user_str)\n",
    "        except Exception:\n",
    "            continue\n",
    "        for rec in lst or []:\n",
    "            try:\n",
    "                src = int(rec.get(\"interact_id\"))\n",
    "            except Exception:\n",
    "                continue\n",
    "            itype = str(rec.get(\"interact_type\", \"\"))\n",
    "            rows.append((src, dst, itype))\n",
    "    edges = pd.DataFrame(rows, columns=[\"src_user_id\", \"dst_user_id\", \"interact_type\"]).astype(\n",
    "        {\"src_user_id\": \"int64\", \"dst_user_id\": \"int64\"}\n",
    "    )\n",
    "\n",
    "    # Remove self-loops\n",
    "    self_mask = edges[\"src_user_id\"] == edges[\"dst_user_id\"]\n",
    "    edges_clean = edges.loc[~self_mask].copy()\n",
    "\n",
    "    # Build graph and compute metrics\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(edges_clean[[\"src_user_id\", \"dst_user_id\"]].itertuples(index=False, name=None))\n",
    "\n",
    "    in_deg = dict(G.in_degree())\n",
    "    out_deg = dict(G.out_degree())\n",
    "\n",
    "    try:\n",
    "        pr = nx.pagerank(G, alpha=0.85, max_iter=100, tol=1e-06)\n",
    "    except nx.PowerIterationFailedConvergence:\n",
    "        pr = nx.pagerank(G, alpha=0.85, max_iter=200, tol=1e-05)\n",
    "\n",
    "    core = nx.core_number(G.to_undirected())\n",
    "\n",
    "    net_feat = pd.DataFrame({\n",
    "        \"user_id\": list(G.nodes()),\n",
    "        \"in_degree\": pd.Series(in_deg),\n",
    "        \"out_degree\": pd.Series(out_deg),\n",
    "        \"pagerank\": pd.Series(pr),\n",
    "        \"kcore\": pd.Series(core),\n",
    "    }).fillna(0)\n",
    "\n",
    "    # Engagement received (excluding self-interactions)\n",
    "    eng = (\n",
    "        edges_clean.groupby([\"dst_user_id\", \"interact_type\"]).size()\n",
    "        .unstack(fill_value=0).rename_axis(None, axis=1)\n",
    "    )\n",
    "    for col in [\"comment\", \"reposts\"]:\n",
    "        if col not in eng.columns:\n",
    "            eng[col] = 0\n",
    "    eng[\"total_engagement\"] = eng[\"comment\"] + eng[\"reposts\"]\n",
    "    eng = eng.reset_index().rename(columns={\"dst_user_id\": \"user_id\"})\n",
    "\n",
    "    # Join with followers/friends\n",
    "    feat = (\n",
    "        eng.merge(prof_df[[\"user_id\", \"user_followers\", \"user_friends\"]], on=\"user_id\", how=\"left\")\n",
    "           .fillna({\"user_followers\": 0, \"user_friends\": 0})\n",
    "    )\n",
    "    feat[\"engagement_per_follower\"] = feat[\"total_engagement\"] / feat[\"user_followers\"].clip(lower=1)\n",
    "\n",
    "    # Save interim files\n",
    "    out_dir = Path(\"data/interim\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    net_feat.to_parquet(out_dir / f\"{prod}_net_features_clean.parquet\", index=False)\n",
    "    feat.to_parquet(out_dir / f\"{prod}_engagement_features_clean.parquet\", index=False)\n",
    "    edges_clean.to_parquet(out_dir / f\"{prod}_edges_clean.parquet\", index=False)\n",
    "\n",
    "    # Small summary line\n",
    "    line = {\n",
    "        \"product\": prod,\n",
    "        \"users\": int(net_feat.shape[0]),\n",
    "        \"edges\": int(edges_clean.shape[0]),\n",
    "        \"engaged_users\": int(feat.shape[0]),\n",
    "        \"self_loops_removed\": int(self_mask.sum()),\n",
    "        \"top_in_degree\": int(net_feat[\"in_degree\"].max()) if not net_feat.empty else 0,\n",
    "    }\n",
    "    batch_summary.append(line)\n",
    "    print(line)\n",
    "\n",
    "# Show batch summary table\n",
    "batch_df = pd.DataFrame(batch_summary).sort_values(\"users\", ascending=False)\n",
    "display(batch_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6444f6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling table\n",
      "--------------\n",
      "rows: 42,224, cols: 21\n",
      "columns (sample): ['comment', 'engagement_per_follower', 'in_degree', 'is_official_influencer', 'kcore', 'log1p_comment', 'log1p_in_degree', 'log1p_out_degree', 'log1p_pagerank', 'log1p_reposts', 'log1p_total_engagement', 'log1p_user_followers'] ...\n",
      "\n",
      "Official influencers present per product:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_in_model</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ruby_face_cream</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electric_toothbrush</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>supor_boosted_showerhead</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intelligent_floor_scrubber</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark_thinking</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abc_reading</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            count_in_model\n",
       "product_name                              \n",
       "ruby_face_cream                         11\n",
       "electric_toothbrush                      8\n",
       "supor_boosted_showerhead                 6\n",
       "intelligent_floor_scrubber               5\n",
       "spark_thinking                           5\n",
       "abc_reading                              4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved: data/processed/features_per_product.parquet\n"
     ]
    }
   ],
   "source": [
    "# Combine per-product features into one modeling table and add product_info flags\n",
    "# Replaces the previous cell end-to-end.\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "PROC_DIR = Path(\"data/processed\")\n",
    "INTERIM_DIR = Path(\"data/interim\")\n",
    "PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load product_info (JSON array) to get official influencers per product\n",
    "info_path = DATA_ROOT / \"product_info.jsonl\"\n",
    "with info_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    info_list = json.load(f)\n",
    "info_df = pd.DataFrame(info_list)\n",
    "\n",
    "def to_int_list(x):\n",
    "    if isinstance(x, list):\n",
    "        out = []\n",
    "        for v in x:\n",
    "            try:\n",
    "                out.append(int(v))\n",
    "            except Exception:\n",
    "                pass\n",
    "        return out\n",
    "    return []\n",
    "\n",
    "info_df[\"influencer_ids\"] = info_df[\"influencer_ids\"].apply(to_int_list)\n",
    "official_map = {row[\"product_name\"]: set(row[\"influencer_ids\"]) for _, row in info_df.iterrows()}\n",
    "\n",
    "# Discover per-product interim files\n",
    "net_files = sorted(INTERIM_DIR.glob(\"*_net_features_clean.parquet\"))\n",
    "\n",
    "all_rows = []\n",
    "for net_path in net_files:\n",
    "    prod = net_path.name.replace(\"_net_features_clean.parquet\", \"\")\n",
    "    eng_path = INTERIM_DIR / f\"{prod}_engagement_features_clean.parquet\"\n",
    "    if not eng_path.exists():\n",
    "        print(f\"Skip {prod}: missing {eng_path.name}\")\n",
    "        continue\n",
    "\n",
    "    net = pd.read_parquet(net_path)\n",
    "    eng = pd.read_parquet(eng_path)\n",
    "\n",
    "    # Merge network + engagement\n",
    "    df = eng.merge(net, on=\"user_id\", how=\"left\")\n",
    "\n",
    "    # Add product name and official influencer flag\n",
    "    df[\"product_name\"] = prod\n",
    "    df[\"is_official_influencer\"] = df[\"user_id\"].astype(\"int64\").isin(official_map.get(prod, set())).astype(\"int8\")\n",
    "\n",
    "    # Stabilized (log1p) versions for skewed counts\n",
    "    for col in [\"user_followers\", \"user_friends\", \"comment\", \"reposts\", \"total_engagement\",\n",
    "                \"in_degree\", \"out_degree\"]:\n",
    "        if col in df.columns:\n",
    "            df[f\"log1p_{col}\"] = np.log1p(df[col].astype(float))\n",
    "\n",
    "    # Safe transform for PageRank (avoid log(0))\n",
    "    pr_series = df.get(\"pagerank\", pd.Series(0.0, index=df.index)).astype(float).fillna(0.0)\n",
    "    df[\"log1p_pagerank\"] = np.log1p(pr_series.clip(lower=1e-12))\n",
    "\n",
    "    all_rows.append(df)\n",
    "\n",
    "if not all_rows:\n",
    "    raise RuntimeError(\"No per-product interim files found. Make sure earlier steps ran.\")\n",
    "\n",
    "features_all = pd.concat(all_rows, ignore_index=True)\n",
    "\n",
    "# Small sanity report\n",
    "print(\"Modeling table\")\n",
    "print(\"--------------\")\n",
    "print(f\"rows: {features_all.shape[0]:,}, cols: {features_all.shape[1]}\")\n",
    "print(\"columns (sample):\", sorted(features_all.columns.tolist())[:12], \"...\")\n",
    "\n",
    "cov = features_all.groupby(\"product_name\")[\"is_official_influencer\"].sum().sort_values(ascending=False)\n",
    "print(\"\\nOfficial influencers present per product:\")\n",
    "display(cov.to_frame(\"count_in_model\"))\n",
    "\n",
    "# Save\n",
    "out_path = PROC_DIR / \"features_per_product.parquet\"\n",
    "features_all.to_parquet(out_path, index=False)\n",
    "print(f\"\\nSaved: {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "influence-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
